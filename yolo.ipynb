{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cpt = 0\n",
    "maxFrames = 85 \n",
    "\n",
    "count=0\n",
    "cap=cv2.VideoCapture(0)\n",
    "while cpt < maxFrames:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame=cv2.resize(frame,(1080,500))\n",
    "    cv2.imshow(\"test window\", frame) \n",
    "    cv2.imwrite(\"F:\\projects\\Crowed_Monetering\\photos\\person_%d.jpg\" %cpt, frame)\n",
    "    time.sleep(0.2)\n",
    "    cpt += 1\n",
    "    if cv2.waitKey(5)&0xFF==27:\n",
    "        break\n",
    "cap.release()   \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"F:\\projects\\Crowed_Monetering\\yolov8s.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1/1: 0... Success  (inf frames of shape 640x480 at 30.00 FPS)\n",
      "\n",
      "\n",
      "    WARNING  stream/video/webcam/dir predict source will accumulate results in RAM unless `stream=True` is passed,\n",
      "    causing potential out-of-memory errors for large sources or long-running streams/videos.\n",
      "\n",
      "    Usage:\n",
      "        results = model(source=..., stream=True)  # generator of Results objects\n",
      "        for r in results:\n",
      "            boxes = r.boxes  # Boxes object for bbox outputs\n",
      "            masks = r.masks  # Masks object for segment masks outputs\n",
      "            probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "0: 480x640 5 persons, 364.4ms\n",
      "0: 480x640 6 persons, 315.3ms\n",
      "0: 480x640 7 persons, 307.8ms\n",
      "0: 480x640 7 persons, 305.4ms\n",
      "0: 480x640 5 persons, 305.8ms\n",
      "0: 480x640 5 persons, 359.0ms\n",
      "0: 480x640 5 persons, 309.4ms\n",
      "0: 480x640 5 persons, 291.2ms\n",
      "0: 480x640 5 persons, 298.7ms\n",
      "0: 480x640 5 persons, 288.7ms\n",
      "0: 480x640 5 persons, 290.2ms\n",
      "0: 480x640 4 persons, 293.1ms\n",
      "0: 480x640 4 persons, 293.2ms\n",
      "0: 480x640 4 persons, 295.1ms\n",
      "0: 480x640 4 persons, 291.3ms\n",
      "0: 480x640 4 persons, 287.9ms\n",
      "0: 480x640 4 persons, 294.3ms\n",
      "0: 480x640 5 persons, 286.4ms\n",
      "0: 480x640 5 persons, 298.7ms\n",
      "0: 480x640 7 persons, 273.0ms\n",
      "0: 480x640 5 persons, 1 cell phone, 285.5ms\n",
      "0: 480x640 6 persons, 1 cell phone, 275.3ms\n",
      "0: 480x640 5 persons, 1 cell phone, 285.4ms\n",
      "0: 480x640 2 persons, 1 cell phone, 280.5ms\n",
      "0: 480x640 3 persons, 3 cell phones, 294.8ms\n",
      "0: 480x640 1 person, 1 cell phone, 296.5ms\n",
      "0: 480x640 3 persons, 1 cell phone, 293.4ms\n",
      "0: 480x640 2 persons, 2 cell phones, 299.9ms\n",
      "0: 480x640 2 persons, 2 cell phones, 303.8ms\n",
      "0: 480x640 2 persons, 1 cell phone, 285.5ms\n",
      "0: 480x640 2 persons, 1 cell phone, 284.4ms\n",
      "0: 480x640 2 persons, 2 cell phones, 290.6ms\n",
      "0: 480x640 2 persons, 2 cell phones, 279.1ms\n",
      "0: 480x640 6 persons, 289.9ms\n",
      "0: 480x640 7 persons, 283.2ms\n",
      "0: 480x640 6 persons, 276.1ms\n",
      "0: 480x640 8 persons, 288.7ms\n",
      "0: 480x640 7 persons, 283.1ms\n",
      "0: 480x640 3 persons, 1 laptop, 272.4ms\n",
      "0: 480x640 6 persons, 1 baseball bat, 1 knife, 282.9ms\n",
      "0: 480x640 6 persons, 1 remote, 295.8ms\n",
      "0: 480x640 8 persons, 273.9ms\n",
      "0: 480x640 6 persons, 1 cell phone, 1 hair drier, 273.8ms\n",
      "0: 480x640 7 persons, 1 bottle, 1 cell phone, 1 hair drier, 272.5ms\n",
      "0: 480x640 8 persons, 1 remote, 1 cell phone, 1 hair drier, 273.7ms\n",
      "0: 480x640 7 persons, 1 bottle, 1 remote, 1 hair drier, 269.5ms\n",
      "0: 480x640 7 persons, 1 skateboard, 1 remote, 1 hair drier, 272.6ms\n",
      "0: 480x640 6 persons, 1 cell phone, 1 hair drier, 270.5ms\n",
      "0: 480x640 5 persons, 1 remote, 1 cell phone, 257.2ms\n",
      "0: 480x640 5 persons, 1 remote, 270.7ms\n",
      "0: 480x640 5 persons, 1 remote, 274.3ms\n",
      "0: 480x640 4 persons, 1 remote, 283.6ms\n",
      "0: 480x640 7 persons, 1 remote, 278.9ms\n",
      "0: 480x640 6 persons, 1 bottle, 1 remote, 1 hair drier, 273.9ms\n",
      "0: 480x640 5 persons, 1 baseball bat, 281.7ms\n",
      "0: 480x640 5 persons, 1 cell phone, 270.8ms\n",
      "0: 480x640 3 persons, 1 book, 273.9ms\n",
      "0: 480x640 4 persons, 275.6ms\n",
      "0: 480x640 4 persons, 1 umbrella, 274.4ms\n",
      "0: 480x640 4 persons, 272.5ms\n",
      "0: 480x640 3 persons, 282.4ms\n",
      "0: 480x640 6 persons, 287.4ms\n",
      "0: 480x640 6 persons, 1 umbrella, 277.7ms\n",
      "0: 480x640 5 persons, 1 cell phone, 286.3ms\n",
      "0: 480x640 3 persons, 1 chair, 294.2ms\n",
      "0: 480x640 7 persons, 261.0ms\n",
      "0: 480x640 6 persons, 274.5ms\n",
      "0: 480x640 5 persons, 269.5ms\n",
      "0: 480x640 6 persons, 286.6ms\n",
      "0: 480x640 7 persons, 281.7ms\n",
      "0: 480x640 7 persons, 315.6ms\n",
      "0: 480x640 7 persons, 300.3ms\n",
      "0: 480x640 8 persons, 303.4ms\n",
      "0: 480x640 8 persons, 299.8ms\n",
      "0: 480x640 7 persons, 301.8ms\n",
      "0: 480x640 7 persons, 277.1ms\n",
      "0: 480x640 6 persons, 273.3ms\n",
      "0: 480x640 6 persons, 1 cell phone, 287.5ms\n",
      "0: 480x640 5 persons, 290.9ms\n",
      "0: 480x640 6 persons, 293.7ms\n",
      "0: 480x640 6 persons, 299.3ms\n",
      "0: 480x640 6 persons, 292.0ms\n",
      "0: 480x640 7 persons, 288.1ms\n",
      "0: 480x640 7 persons, 272.1ms\n",
      "0: 480x640 8 persons, 303.2ms\n",
      "0: 480x640 7 persons, 1 cell phone, 301.5ms\n",
      "0: 480x640 7 persons, 1 remote, 296.8ms\n",
      "0: 480x640 9 persons, 1 cell phone, 301.7ms\n",
      "0: 480x640 7 persons, 1 remote, 270.4ms\n",
      "0: 480x640 5 persons, 295.3ms\n",
      "0: 480x640 6 persons, 306.4ms\n",
      "0: 480x640 5 persons, 320.3ms\n",
      "0: 480x640 4 persons, 302.5ms\n",
      "0: 480x640 5 persons, 333.8ms\n",
      "0: 480x640 5 persons, 338.7ms\n",
      "0: 480x640 5 persons, 346.4ms\n",
      "0: 480x640 6 persons, 294.0ms\n",
      "0: 480x640 5 persons, 477.6ms\n",
      "0: 480x640 4 persons, 332.0ms\n",
      "0: 480x640 4 persons, 1 toothbrush, 300.9ms\n",
      "0: 480x640 4 persons, 303.2ms\n",
      "0: 480x640 5 persons, 316.4ms\n",
      "0: 480x640 5 persons, 353.5ms\n",
      "0: 480x640 6 persons, 401.7ms\n",
      "0: 480x640 6 persons, 305.9ms\n",
      "0: 480x640 4 persons, 303.5ms\n",
      "0: 480x640 5 persons, 358.3ms\n",
      "0: 480x640 4 persons, 347.6ms\n",
      "0: 480x640 5 persons, 321.8ms\n",
      "0: 480x640 4 persons, 318.3ms\n",
      "0: 480x640 5 persons, 307.7ms\n",
      "0: 480x640 5 persons, 369.5ms\n",
      "0: 480x640 6 persons, 315.5ms\n",
      "0: 480x640 6 persons, 300.2ms\n",
      "0: 480x640 6 persons, 299.4ms\n",
      "0: 480x640 6 persons, 292.6ms\n",
      "0: 480x640 6 persons, 328.9ms\n",
      "0: 480x640 7 persons, 322.9ms\n",
      "0: 480x640 6 persons, 301.6ms\n",
      "0: 480x640 6 persons, 300.5ms\n",
      "0: 480x640 5 persons, 354.1ms\n",
      "0: 480x640 5 persons, 322.8ms\n",
      "0: 480x640 5 persons, 286.6ms\n",
      "0: 480x640 5 persons, 307.9ms\n",
      "0: 480x640 5 persons, 293.6ms\n",
      "0: 480x640 5 persons, 286.8ms\n",
      "0: 480x640 5 persons, 315.9ms\n",
      "0: 480x640 6 persons, 306.1ms\n",
      "0: 480x640 6 persons, 285.4ms\n",
      "0: 480x640 6 persons, 282.3ms\n",
      "0: 480x640 6 persons, 289.2ms\n",
      "0: 480x640 4 persons, 292.6ms\n",
      "0: 480x640 4 persons, 303.0ms\n",
      "0: 480x640 5 persons, 290.1ms\n",
      "0: 480x640 6 persons, 302.0ms\n",
      "0: 480x640 5 persons, 325.0ms\n",
      "0: 480x640 6 persons, 294.8ms\n",
      "0: 480x640 6 persons, 1 hair drier, 363.8ms\n",
      "0: 480x640 5 persons, 307.8ms\n",
      "0: 480x640 5 persons, 1 horse, 294.0ms\n",
      "0: 480x640 4 persons, 1 horse, 305.6ms\n",
      "0: 480x640 4 persons, 1 horse, 301.7ms\n",
      "0: 480x640 5 persons, 1 horse, 291.0ms\n",
      "0: 480x640 5 persons, 1 hair drier, 286.7ms\n",
      "0: 480x640 5 persons, 1 hair drier, 315.7ms\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(source=\"0\",show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['dog','person','cat','tv','car','meatballs','marinara sauce','tomato soup','chicken noodle soup','french onion soup','chicken breast','ribs','pulled pork','hamburger','cavity','Shivam','Priyanshu_scalar','Priyanav'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for r in results:\n",
    "    for c in r.boxes.cls:\n",
    "        if names[int(c)]==\"Priyanav\":\n",
    "            engine.say(\"Priynav kuttaa hai\")\n",
    "            engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "engine.say(\"I will speak this text\")\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 608x800 1 Priyanshu_scalar, 477.3ms\n",
      "Speed: 0.0ms preprocess, 477.3ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 608x800 1 Priyanshu_scalar, 399.9ms\n",
      "Speed: 0.0ms preprocess, 399.9ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 608x800 1 Priyanav, 397.0ms\n",
      "Speed: 0.0ms preprocess, 397.0ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 608x800 1 Priyanav, 394.3ms\n",
      "Speed: 0.0ms preprocess, 394.3ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 608x800 1 Priyanshu_scalar, 388.5ms\n",
      "Speed: 0.0ms preprocess, 388.5ms inference, 6.1ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 608x800 1 Priyanshu_scalar, 394.7ms\n",
      "Speed: 0.0ms preprocess, 394.7ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 608x800 1 Shivam, 1 Priyanav, 389.1ms\n",
      "Speed: 8.0ms preprocess, 389.1ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 608x800 1 Shivam, 1 Priyanshu_scalar, 384.4ms\n",
      "Speed: 4.3ms preprocess, 384.4ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 608x800 1 Priyanshu_scalar, 404.6ms\n",
      "Speed: 1.9ms preprocess, 404.6ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened():\n\u001b[0;32m      4\u001b[0m   ret,frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m----> 6\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mcls:\n",
      "File \u001b[1;32mc:\\Users\\bhard\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:110\u001b[0m, in \u001b[0;36mYOLO.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bhard\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bhard\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\yolo\\engine\\model.py:248\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, overrides)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bhard\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\yolo\\engine\\predictor.py:150\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bhard\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bhard\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\yolo\\engine\\predictor.py:212\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# inference\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 212\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# postprocess\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt[\u001b[38;5;241m2\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\bhard\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bhard\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:312\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize)\u001b[0m\n\u001b[0;32m    309\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 312\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize) \u001b[38;5;28;01mif\u001b[39;00m augment \u001b[38;5;129;01mor\u001b[39;00m visualize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[1;32mc:\\Users\\bhard\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\bhard\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\nn\\tasks.py:203\u001b[0m, in \u001b[0;36mDetectionModel.forward\u001b[1;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_augment(x)  \u001b[38;5;66;03m# augmented inference, None\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bhard\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\nn\\tasks.py:59\u001b[0m, in \u001b[0;36mBaseModel._forward_once\u001b[1;34m(self, x, profile, visualize)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m m(x)  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n\u001b[0;32m     61\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisualize feature not yet supported\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "  ret,frame = cap.read()\n",
    "\n",
    "  result = model(frame)\n",
    "\n",
    "  # for r in results:\n",
    "  #   for c in r.boxes.cls:\n",
    "  #      if names[int(c)]==\"Shivam\":\n",
    "         \n",
    "  #        engine.say(\"Shivam\")\n",
    "  #        engine.runAndWait()\n",
    "  #      else :\n",
    "  #        pass  \n",
    "        \n",
    "      \n",
    "         \n",
    "  if cv2.waitKey(10) & 0xFF == ord(\"c\"):\n",
    "        break\n",
    "  time.sleep(1)   \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['dog','person','cat','tv','car','meatballs','marinara sauce','tomato soup','chicken noodle soup','french onion soup','chicken breast','ribs','pulled pork','hamburger','cavity','Shivam','Priyanshu_scalar','Priyanav']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap= cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 416x800 (no detections), 399.4ms\n",
      "Speed: 2.7ms preprocess, 399.4ms inference, 2.5ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 (no detections), 445.8ms\n",
      "Speed: 1.0ms preprocess, 445.8ms inference, 5.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 (no detections), 335.3ms\n",
      "Speed: 1.0ms preprocess, 335.3ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 (no detections), 330.1ms\n",
      "Speed: 2.0ms preprocess, 330.1ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 (no detections), 348.3ms\n",
      "Speed: 1.0ms preprocess, 348.3ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 (no detections), 344.6ms\n",
      "Speed: 0.0ms preprocess, 344.6ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 (no detections), 357.3ms\n",
      "Speed: 1.0ms preprocess, 357.3ms inference, 2.1ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Priyanav, 457.4ms\n",
      "Speed: 2.0ms preprocess, 457.4ms inference, 8.4ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 (no detections), 367.8ms\n",
      "Speed: 2.1ms preprocess, 367.8ms inference, 1.3ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Priyanav, 474.8ms\n",
      "Speed: 2.0ms preprocess, 474.8ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 396.6ms\n",
      "Speed: 1.0ms preprocess, 396.6ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Priyanav, 409.0ms\n",
      "Speed: 2.0ms preprocess, 409.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 (no detections), 366.3ms\n",
      "Speed: 1.0ms preprocess, 366.3ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 361.7ms\n",
      "Speed: 1.5ms preprocess, 361.7ms inference, 2.1ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 377.4ms\n",
      "Speed: 1.0ms preprocess, 377.4ms inference, 1.2ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 341.3ms\n",
      "Speed: 0.0ms preprocess, 341.3ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 (no detections), 330.1ms\n",
      "Speed: 2.2ms preprocess, 330.1ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 366.2ms\n",
      "Speed: 1.0ms preprocess, 366.2ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Priyanav, 336.0ms\n",
      "Speed: 1.2ms preprocess, 336.0ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 (no detections), 382.3ms\n",
      "Speed: 1.0ms preprocess, 382.3ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Priyanav, 361.4ms\n",
      "Speed: 1.0ms preprocess, 361.4ms inference, 1.2ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Priyanav, 338.9ms\n",
      "Speed: 1.0ms preprocess, 338.9ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 335.7ms\n",
      "Speed: 1.1ms preprocess, 335.7ms inference, 1.3ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Priyanav, 331.6ms\n",
      "Speed: 1.0ms preprocess, 331.6ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 354.6ms\n",
      "Speed: 1.0ms preprocess, 354.6ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 367.6ms\n",
      "Speed: 1.0ms preprocess, 367.6ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 355.5ms\n",
      "Speed: 1.0ms preprocess, 355.5ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 (no detections), 331.1ms\n",
      "Speed: 1.0ms preprocess, 331.1ms inference, 0.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Priyanav, 308.7ms\n",
      "Speed: 1.0ms preprocess, 308.7ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Priyanav, 328.4ms\n",
      "Speed: 1.0ms preprocess, 328.4ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Priyanav, 364.5ms\n",
      "Speed: 1.1ms preprocess, 364.5ms inference, 1.1ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Priyanav, 358.8ms\n",
      "Speed: 3.0ms preprocess, 358.8ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 1 Priyanav, 309.2ms\n",
      "Speed: 0.0ms preprocess, 309.2ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 289.4ms\n",
      "Speed: 1.0ms preprocess, 289.4ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 286.2ms\n",
      "Speed: 1.0ms preprocess, 286.2ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 1 Priyanav, 274.8ms\n",
      "Speed: 0.0ms preprocess, 274.8ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 292.7ms\n",
      "Speed: 2.0ms preprocess, 292.7ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 1 Priyanav, 294.5ms\n",
      "Speed: 1.0ms preprocess, 294.5ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 351.2ms\n",
      "Speed: 1.0ms preprocess, 351.2ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 1 Priyanav, 374.7ms\n",
      "Speed: 1.2ms preprocess, 374.7ms inference, 2.2ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 317.5ms\n",
      "Speed: 1.1ms preprocess, 317.5ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 316.2ms\n",
      "Speed: 0.1ms preprocess, 316.2ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 291.0ms\n",
      "Speed: 1.0ms preprocess, 291.0ms inference, 1.2ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 322.4ms\n",
      "Speed: 1.1ms preprocess, 322.4ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 367.9ms\n",
      "Speed: 1.0ms preprocess, 367.9ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 358.8ms\n",
      "Speed: 1.0ms preprocess, 358.8ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 325.6ms\n",
      "Speed: 0.0ms preprocess, 325.6ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 309.4ms\n",
      "Speed: 1.0ms preprocess, 309.4ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 322.8ms\n",
      "Speed: 1.0ms preprocess, 322.8ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 340.4ms\n",
      "Speed: 1.0ms preprocess, 340.4ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 342.6ms\n",
      "Speed: 1.0ms preprocess, 342.6ms inference, 1.4ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 323.2ms\n",
      "Speed: 1.0ms preprocess, 323.2ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 371.7ms\n",
      "Speed: 0.0ms preprocess, 371.7ms inference, 2.1ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 340.6ms\n",
      "Speed: 1.0ms preprocess, 340.6ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 336.6ms\n",
      "Speed: 2.3ms preprocess, 336.6ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 363.2ms\n",
      "Speed: 2.0ms preprocess, 363.2ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 362.2ms\n",
      "Speed: 3.0ms preprocess, 362.2ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 340.1ms\n",
      "Speed: 2.5ms preprocess, 340.1ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 326.9ms\n",
      "Speed: 1.1ms preprocess, 326.9ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 351.8ms\n",
      "Speed: 1.0ms preprocess, 351.8ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 364.8ms\n",
      "Speed: 2.0ms preprocess, 364.8ms inference, 2.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 1 Priyanav, 334.2ms\n",
      "Speed: 1.0ms preprocess, 334.2ms inference, 1.1ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 299.6ms\n",
      "Speed: 1.2ms preprocess, 299.6ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n",
      "\n",
      "0: 416x800 1 Shivam, 308.5ms\n",
      "Speed: 1.0ms preprocess, 308.5ms inference, 1.0ms postprocess per image at shape (1, 3, 800, 800)\n"
     ]
    }
   ],
   "source": [
    "while True:    \n",
    "    ret,frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "#    count += 1\n",
    "#    if count % 3 != 0:\n",
    "#        continue\n",
    "    frame=cv2.resize(frame,(1020,500))\n",
    "   \n",
    "\n",
    "    results=model.predict(frame)\n",
    " #   print(results)\n",
    "    a=results[0].boxes.data\n",
    "    px=pd.DataFrame(a).astype(\"float\")\n",
    "#    print(px)\n",
    "    \n",
    "   \n",
    "    for index,row in px.iterrows():\n",
    "#        print(row)\n",
    " \n",
    "        x1=int(row[0])\n",
    "        y1=int(row[1])\n",
    "        x2=int(row[2])\n",
    "        y2=int(row[3])\n",
    "        d=int(row[5])\n",
    "        \n",
    "        c=names[d]\n",
    "        \n",
    "\n",
    "   \n",
    "\n",
    "    cv2.imshow(\"RGB\", frame)\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"c\"):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
